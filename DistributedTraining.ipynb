{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94e798ec-9f22-41d0-9ebd-b6feba54a350",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Minimal PyTorch examples for different parallelism strategies:\n",
    "1. DDP (Distributed Data Parallel)\n",
    "2. FSDP 2 (Fully Sharded Data Parallel v2)\n",
    "3. Tensor Parallelism\n",
    "4. Sequence/Context Parallelism\n",
    "\n",
    "Run with: torchrun --nproc_per_node=4 parallelism_examples.py --strategy ddp\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "from torch.distributed.tensor.parallel import (\n",
    "    ColwiseParallel, RowwiseParallel, parallelize_module\n",
    ")\n",
    "from torch.distributed.tensor import DeviceMesh, distribute_tensor, Replicate, Shard\n",
    "import argparse\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22cfd41e-b732-425c-a3e7-daec29b467b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size=1000, d_model=512, nhead=8, num_layers=6, seq_len=128):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(seq_len, d_model))\n",
    "        \n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=d_model * 4,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.classifier = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len)\n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        x = self.embedding(x) + self.pos_encoding[:seq_len].unsqueeze(0)\n",
    "        \n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bda3bbb9-d68f-42d7-8955-5b0a5a2b0c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_distributed():\n",
    "    \"\"\"Initialize distributed training environment\"\"\"\n",
    "    if dist.is_initialized():\n",
    "        dist.destroy_process_group()\n",
    "    dist.init_process_group(backend=\"gloo\")\n",
    "    rank = dist.get_rank()\n",
    "    world_size = dist.get_world_size()\n",
    "\n",
    "    device_str = \"cpu\"\n",
    "    count = torch.cpu.device_count()\n",
    "    if torch.cuda.is_available():\n",
    "        device_str = \"cuda\"\n",
    "        count = torch.cuda.device_count()\n",
    "    # elif torch.backends.mps.is_available():\n",
    "    #     device_str = \"mps\"\n",
    "    #     count = torch.mps.device_count()\n",
    "    device = torch.device(f\"{device_str}:{rank % count}\")\n",
    "    torch.set_default_device(device)\n",
    "    return rank, world_size, device\n",
    "\n",
    "\n",
    "def create_dummy_data(batch_size, seq_len, vocab_size, device):\n",
    "    \"\"\"Create dummy training data\"\"\"\n",
    "    inputs = torch.randint(0, vocab_size, (batch_size, seq_len), device=device)\n",
    "    targets = inputs[:, 1:].contiguous()\n",
    "    inputs = inputs[:, :-1].contiguous()\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4586b189-8240-4c29-886d-7bda8a69263c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '12355'\n",
    "os.environ['RANK'] = '0'\n",
    "os.environ['WORLD_SIZE'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb78f7ff-98a9-4c61-ac79-608b5f295d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. DDP (Distributed Data Parallel)\n",
    "# =============================================================================\n",
    "def ddp_example():\n",
    "    \"\"\"DDP replicates model on each GPU, synchronizes gradients\"\"\"\n",
    "    rank, world_size, device = setup_distributed()\n",
    "    \n",
    "    model = SimpleTransformer().to(device)\n",
    "    model = DDP(model, device_ids=[device])\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    for step in range(5):\n",
    "        batch_size = 4\n",
    "        inputs, targets = create_dummy_data(batch_size, 128, 1000, device)\n",
    "        \n",
    "        outputs = model(inputs)  # Shape: (batch, seq_len, vocab_size)\n",
    "        \n",
    "        loss = F.cross_entropy(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if rank == 0:\n",
    "            print(f\"DDP Step {step}: Loss = {loss.item():.4f}\")\n",
    "    \n",
    "    dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f6aa0a69-5339-43e8-87fb-15749e534778",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "DistributedDataParallel device_ids and output_device arguments only work with single-device/multiple-device GPU modules or CPU modules, but got device_ids [device(type='cpu', index=0)], output_device None, and module parameters {device(type='cpu')}.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mddp_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mddp_example\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      6\u001b[39m rank, world_size, device = setup_distributed()\n\u001b[32m      8\u001b[39m model = SimpleTransformer().to(device)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m model = \u001b[43mDDP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m optimizer = torch.optim.Adam(model.parameters(), lr=\u001b[32m1e-4\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m5\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/transformers/garcon/.pixi/envs/default/lib/python3.12/site-packages/torch/nn/parallel/distributed.py:749\u001b[39m, in \u001b[36mDistributedDataParallel.__init__\u001b[39m\u001b[34m(self, module, device_ids, output_device, dim, broadcast_buffers, init_sync, process_group, bucket_cap_mb, find_unused_parameters, check_reduction, gradient_as_bucket_view, static_graph, delay_all_reduce_named_params, param_to_hook_all_reduce, mixed_precision, device_mesh)\u001b[39m\n\u001b[32m    742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    743\u001b[39m     device_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    744\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(device_ids) == \u001b[32m0\u001b[39m  \u001b[38;5;66;03m# For backward compatibility.\u001b[39;00m\n\u001b[32m    745\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device_type == \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    746\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_multi_device_module\n\u001b[32m    747\u001b[39m ):\n\u001b[32m    748\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m device_ids \u001b[38;5;129;01mor\u001b[39;00m output_device:\n\u001b[32m--> \u001b[39m\u001b[32m749\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_log_and_throw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    750\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;167;43;01mValueError\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDistributedDataParallel device_ids and output_device arguments \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    752\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43monly work with single-device/multiple-device GPU modules or CPU modules, \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    753\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbut got device_ids \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdevice_ids\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m, output_device \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43moutput_device\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m, \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    754\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mand module parameters \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mp\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_module_parameters\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    755\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    757\u001b[39m     \u001b[38;5;28mself\u001b[39m.device_ids = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    758\u001b[39m     \u001b[38;5;28mself\u001b[39m.output_device = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/transformers/garcon/.pixi/envs/default/lib/python3.12/site-packages/torch/nn/parallel/distributed.py:1135\u001b[39m, in \u001b[36mDistributedDataParallel._log_and_throw\u001b[39m\u001b[34m(self, err_type, err_msg)\u001b[39m\n\u001b[32m   1133\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.logger \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1134\u001b[39m     \u001b[38;5;28mself\u001b[39m.logger.set_error_and_log(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(err_type)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1135\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m err_type(err_msg)\n",
      "\u001b[31mValueError\u001b[39m: DistributedDataParallel device_ids and output_device arguments only work with single-device/multiple-device GPU modules or CPU modules, but got device_ids [device(type='cpu', index=0)], output_device None, and module parameters {device(type='cpu')}."
     ]
    }
   ],
   "source": [
    "ddp_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc9dcf8-db6e-4a6b-9ec7-e8656f5a1127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2. FSDP 2 (Fully Sharded Data Parallel)\n",
    "# =============================================================================\n",
    "def fsdp_example():\n",
    "    \"\"\"FSDP 2 shards model parameters across GPUs using manual wrapping\"\"\"\n",
    "    rank, world_size, device = setup_distributed()\n",
    "    \n",
    "    # Create model with manual FSDP wrapping\n",
    "    model = SimpleTransformer().to(device)\n",
    "    \n",
    "    # FSDP 2 uses manual wrapping - wrap each transformer layer individually\n",
    "    # This gives more control over sharding strategy\n",
    "    for i, layer in enumerate(model.transformer.layers):\n",
    "        model.transformer.layers[i] = FSDP(\n",
    "            layer,\n",
    "            device_id=device,\n",
    "        )\n",
    "    \n",
    "    # Wrap the entire model as well\n",
    "    model = FSDP(\n",
    "        model,\n",
    "        device_id=device,\n",
    "    )\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    # Training loop\n",
    "    for step in range(5):\n",
    "        batch_size = 4\n",
    "        inputs, targets = create_dummy_data(batch_size, 128, 1000, device)\n",
    "        \n",
    "        # Forward pass - FSDP handles all-gather internally\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Loss\n",
    "        loss = F.cross_entropy(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "        \n",
    "        # Backward pass - FSDP handles reduce-scatter internally  \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if rank == 0:\n",
    "            print(f\"FSDP Step {step}: Loss = {loss.item():.4f}\")\n",
    "    \n",
    "    dist.destroy_process_group()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Garcon (pixi)",
   "language": "python",
   "name": "garcon-pixi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
