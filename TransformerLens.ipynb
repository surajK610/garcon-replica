{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b25a810-c7b1-4ae3-b1a7-cedcb7abb3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_lens\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.utils import to_numpy\n",
    "import nnsight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0379842-6b61-44a7-9fa1-bdf0712a0594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model_name = 'gpt2-small'\n",
    "model = HookedTransformer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3951f080-cde6-4345-ae87-cbc70494f41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neuron_acts(text, layer, neuron_index):\n",
    "    cache = {}\n",
    "    def cache_hooks(act, hook):\n",
    "        cache['activation'] = act[0, :, neuron_index]\n",
    "\n",
    "    model.run_with_hooks(text, fwd_hooks=[(f\"blocks.{layer}.mlp.hook_post\", cache_hooks)])\n",
    "    return to_numpy(cache[\"activation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ef649d5-9d8e-43d4-909c-71dfff6b2dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', 'The', ' following', ' is', ' a', ' list', ' of', ' powers', ' of', ' 10', ':', ' 1', ',', ' 10', ',', ' 100', ',', ' 1000', ',', ' 10000']\n"
     ]
    }
   ],
   "source": [
    "default_layer = 9\n",
    "default_neuron_index = 652\n",
    "default_text = \"The following is a list of powers of 10: 1, 10, 100, 1000, 10000\"\n",
    "\n",
    "print(model.to_str_tokens(default_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "004f0010-25be-4771-ba43-d5d93f3200d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.08643513 -0.14071983 -0.10398179 -0.12390769 -0.04058974 -0.11064921\n",
      " -0.05189849 -0.11276147 -0.0690545  -0.11189394 -0.03059204 -0.10336912\n",
      " -0.04322357  1.59356    -0.1420576   2.5116599  -0.13316406  2.5196717\n",
      " -0.11360858  3.076529  ]\n"
     ]
    }
   ],
   "source": [
    "print(get_neuron_acts(default_text, default_layer, default_neuron_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "173f3345-f88b-468d-9a29-063fc4002f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "style_string = \"\"\"<style> \n",
    "    span.token {\n",
    "        border: 1px solid rgb(123, 123, 123)\n",
    "        } \n",
    "    </style>\"\"\"\n",
    "\n",
    "\n",
    "def calculate_color(val, max_val, min_val):\n",
    "    # Hacky code that takes in a value val in range [min_val, max_val], normalizes it to [0, 1] and returns a color which interpolates between slightly off-white and red (0 = white, 1 = red)\n",
    "    # We return a string of the form \"rgb(240, 240, 240)\" which is a color CSS knows\n",
    "    normalized_val = (val - min_val) / max_val\n",
    "    return f\"rgb(240, {240*(1-normalized_val)}, {240*(1-normalized_val)})\"\n",
    "\n",
    "\n",
    "def basic_neuron_vis(text, layer, neuron_index, max_val=None, min_val=None):\n",
    "    \"\"\"\n",
    "    text: The text to visualize\n",
    "    layer: The layer index\n",
    "    neuron_index: The neuron index\n",
    "    max_val: The top end of our activation range, defaults to the maximum activation\n",
    "    min_val: The top end of our activation range, defaults to the minimum activation\n",
    "\n",
    "    Returns a string of HTML that displays the text with each token colored according to its activation\n",
    "\n",
    "    Note: It's useful to be able to input a fixed max_val and min_val, because otherwise the colors will change as you edit the text, which is annoying.\n",
    "    \"\"\"\n",
    "    if layer is None:\n",
    "        return \"Please select a Layer\"\n",
    "    if neuron_index is None:\n",
    "        return \"Please select a Neuron\"\n",
    "    acts = get_neuron_acts(text, layer, neuron_index)\n",
    "    act_max = acts.max()\n",
    "    act_min = acts.min()\n",
    "    # Defaults to the max and min of the activations\n",
    "    if max_val is None:\n",
    "        max_val = act_max\n",
    "    if min_val is None:\n",
    "        min_val = act_min\n",
    "    # We want to make a list of HTML strings to concatenate into our final HTML string\n",
    "    # We first add the style to make each token element have a nice border\n",
    "    htmls = [style_string]\n",
    "    # We then add some text to tell us what layer and neuron we're looking at - we're just dealing with strings and can use f-strings as normal\n",
    "    # h4 means \"small heading\"\n",
    "    htmls.append(f\"<h4>Layer: <b>{layer}</b>. Neuron Index: <b>{neuron_index}</b></h4>\")\n",
    "    # We then add a line telling us the limits of our range\n",
    "    htmls.append(\n",
    "        f\"<h4>Max Range: <b>{max_val:.4f}</b>. Min Range: <b>{min_val:.4f}</b></h4>\"\n",
    "    )\n",
    "    # If we added a custom range, print a line telling us the range of our activations too.\n",
    "    if act_max != max_val or act_min != min_val:\n",
    "        htmls.append(\n",
    "            f\"<h4>Custom Range Set. Max Act: <b>{act_max:.4f}</b>. Min Act: <b>{act_min:.4f}</b></h4>\"\n",
    "        )\n",
    "    # Convert the text to a list of tokens\n",
    "    str_tokens = model.to_str_tokens(text)\n",
    "    for tok, act in zip(str_tokens, acts):\n",
    "        # A span is an HTML element that lets us style a part of a string (and remains on the same line by default)\n",
    "        # We set the background color of the span to be the color we calculated from the activation\n",
    "        # We set the contents of the span to be the token\n",
    "        htmls.append(\n",
    "            f\"<span class='token' style='background-color:{calculate_color(act, max_val, min_val)}' >{tok}</span>\"\n",
    "        )\n",
    "\n",
    "    return \"\".join(htmls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8a279eb-055a-4093-9e98-80888d7b6885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displayed HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style> \n",
       "    span.token {\n",
       "        border: 1px solid rgb(123, 123, 123)\n",
       "        } \n",
       "    </style><h4>Layer: <b>9</b>. Neuron Index: <b>652</b></h4><h4>Max Range: <b>4.0000</b>. Min Range: <b>0.0000</b></h4><h4>Custom Range Set. Max Act: <b>3.0765</b>. Min Act: <b>-0.1421</b></h4><span class='token' style='background-color:rgb(240, 245.18612670898438, 245.18612670898438)' ><|endoftext|></span><span class='token' style='background-color:rgb(240, 248.4431915283203, 248.4431915283203)' >The</span><span class='token' style='background-color:rgb(240, 246.23892211914062, 246.23892211914062)' > following</span><span class='token' style='background-color:rgb(240, 247.4344482421875, 247.4344482421875)' > is</span><span class='token' style='background-color:rgb(240, 242.43539428710938, 242.43539428710938)' > a</span><span class='token' style='background-color:rgb(240, 246.63894653320312, 246.63894653320312)' > list</span><span class='token' style='background-color:rgb(240, 243.11390686035156, 243.11390686035156)' > of</span><span class='token' style='background-color:rgb(240, 246.76568603515625, 246.76568603515625)' > powers</span><span class='token' style='background-color:rgb(240, 244.14328002929688, 244.14328002929688)' > of</span><span class='token' style='background-color:rgb(240, 246.71365356445312, 246.71365356445312)' > 10</span><span class='token' style='background-color:rgb(240, 241.83551025390625, 241.83551025390625)' >:</span><span class='token' style='background-color:rgb(240, 246.2021484375, 246.2021484375)' > 1</span><span class='token' style='background-color:rgb(240, 242.59339904785156, 242.59339904785156)' >,</span><span class='token' style='background-color:rgb(240, 144.3863983154297, 144.3863983154297)' > 10</span><span class='token' style='background-color:rgb(240, 248.5234375, 248.5234375)' >,</span><span class='token' style='background-color:rgb(240, 89.30040740966797, 89.30040740966797)' > 100</span><span class='token' style='background-color:rgb(240, 247.98983764648438, 247.98983764648438)' >,</span><span class='token' style='background-color:rgb(240, 88.8197021484375, 88.8197021484375)' > 1000</span><span class='token' style='background-color:rgb(240, 246.81649780273438, 246.81649780273438)' >,</span><span class='token' style='background-color:rgb(240, 55.40825653076172, 55.40825653076172)' > 10000</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NBVAL_IGNORE_OUTPUT\n",
    "# The function outputs a string of HTML\n",
    "default_max_val = 4.0\n",
    "default_min_val = 0.0\n",
    "default_html_string = basic_neuron_vis(\n",
    "    default_text,\n",
    "    default_layer,\n",
    "    default_neuron_index,\n",
    "    max_val=default_max_val,\n",
    "    min_val=default_min_val,\n",
    ")\n",
    "\n",
    "# IPython lets us display HTML\n",
    "print(\"Displayed HTML\")\n",
    "display(HTML(default_html_string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "841b233e-b897-48ff-ab58-03f5047b7811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.HTML(value=f\"Hacky Interactive Neuroscope for {model_name}\")\n",
    "    # The input elements\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            text = gr.Textbox(label=\"Text\", value=default_text)\n",
    "            # Precision=0 makes it an int, otherwise it's a float\n",
    "            # Value sets the initial default value\n",
    "            layer = gr.Number(label=\"Layer\", value=default_layer, precision=0)\n",
    "            neuron_index = gr.Number(\n",
    "                label=\"Neuron Index\", value=default_neuron_index, precision=0\n",
    "            )\n",
    "            # If empty, these two map to None\n",
    "            max_val = gr.Number(label=\"Max Value\", value=default_max_val)\n",
    "            min_val = gr.Number(label=\"Min Value\", value=default_min_val)\n",
    "            inputs = [text, layer, neuron_index, max_val, min_val]\n",
    "        with gr.Column():\n",
    "            # The output element\n",
    "            out = gr.HTML(label=\"Neuron Acts\", value=default_html_string)\n",
    "    for inp in inputs:\n",
    "        inp.change(basic_neuron_vis, inputs, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bccb68ff-cf3f-44a4-81fd-1ee819c0cc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://17241cd1236345e76f.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://17241cd1236345e76f.gradio.live\" width=\"100%\" height=\"1000\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nv/n0srzmxd07lck823c1rsxl8w0000gn/T/ipykernel_82152/2880486167.py:13: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  normalized_val = (val - min_val) / max_val\n"
     ]
    }
   ],
   "source": [
    "# NBVAL_IGNORE_OUTPUT\n",
    "demo.launch(share=True, height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90633823-67cd-4b0e-b696-bc4e9da9e285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7dec2c-56d8-4ad9-8925-2e4c0989f4f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Garcon (pixi)",
   "language": "python",
   "name": "garcon-pixi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
